{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58471bb",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334428e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import json\n",
    "import csv\n",
    "from sklearn.feature_extraction import text\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc14d26",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b1b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset constants ==============================\n",
    "N = 331675   # Number of points\n",
    "D = 7   # Number of (useful) features\n",
    "C = 172    # Number of unique categories\n",
    "mC = 1508    # Largest label coding (plus one for array creation purposes)\n",
    "M = 12    # Number of unique months\n",
    "\n",
    "class NaiveBayes(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load test file\n",
    "        csv_file = open('KickStarterData_nb.csv', encoding=\"utf-8\")\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        \n",
    "        data_arr = np.empty((N, D + 1))    # D + 1 since need to account for the labels\n",
    "        counter = -2    # For skipping the row of feature names \n",
    "        for row in csv_reader:\n",
    "            counter += 1\n",
    "            if (counter == -1):\n",
    "                continue\n",
    "                            \n",
    "            temp_arr = np.array(row)\n",
    "            data_arr[counter] = temp_arr[1:]    # Ignores the ID vaules\n",
    "        \n",
    "        # Splits the data between data points and labels\n",
    "        self.x = data_arr[:, 0:D]\n",
    "        self.y = data_arr[:, D]\n",
    "        \n",
    "        # Set all other instance variables to None\n",
    "        self.priors = None\n",
    "        self.likelihoods_cont_0 = None\n",
    "        self.likelihoods_cont_1 = None\n",
    "        self.likelihood_cat_0 = None\n",
    "        self.likelihood_cat_1 = None\n",
    "        self.likelihood_mon_0 = None\n",
    "        self.likelihood_mon_1 = None\n",
    "        \n",
    "    def find_priors(self, y_train):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "            y_train: (N,) numpy array, labels for the points in the training set\n",
    "        SETS:\n",
    "            self.priors: (2,) numpy array, the probabilities of having a label of 0 or 1, P(y)\n",
    "        \"\"\"\n",
    "        # Get total number of labels\n",
    "        N_train = np.shape(y_train)[0]\n",
    "        \n",
    "        # Get the number of each label\n",
    "        num_1 = y_train.sum()\n",
    "        num_0 = N_train - num_1\n",
    "        \n",
    "        # Calculate each prior\n",
    "        priors = np.array([num_0, num_1])\n",
    "        priors = priors / N_train\n",
    "        \n",
    "        self.priors = priors\n",
    "\n",
    "    def find_likelihoods_cont(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Finds the mean and variance of each continuous feature (so all features minus category) to be\n",
    "        able to use a Guassian estimation.\n",
    "        \n",
    "        ARGS:\n",
    "            x_train: (N, D) numpy array, data set for the model to be trained on\n",
    "            y_train: (N,) numpy array, labels for x_train\n",
    "        SETS:\n",
    "            self.likelihoods_cont_0: (D - 2, 2) numpy array, the means ([:, 0]) and the variance\n",
    "                                     ([:, 1]) of each of the D continuous features for y = 0, P(x|y = 0)\n",
    "            self.likelihoods_cont_1: (D - 2, 2) numpy array, the means ([:, 0]) and the variance\n",
    "                                     ([:, 1]) of each of the D continuous features for y = 1, P(x|y = 1)\n",
    "        \"\"\"\n",
    "        # Transpose the labels to be vertical\n",
    "        y_train_vert = np.array([y_train]).T\n",
    "        \n",
    "        # Get the continuous data points for each label\n",
    "        x_train_1 = x_train * y_train_vert\n",
    "        x_train_1 = x_train_1[~np.all(x_train_1 == 0, axis=1)]\n",
    "        x_train_0 = x_train * (1 - y_train_vert)\n",
    "        x_train_0 = x_train_0[~np.all(x_train_0 == 0, axis=1)]\n",
    "        \n",
    "        # Take out the ordinal data\n",
    "        x_train_1 = x_train_1[:, 2:]\n",
    "        x_train_0 = x_train_0[:, 2:]\n",
    "        \n",
    "        # Get the means and variances for each subset of the training set\n",
    "        avg_0 = np.average(x_train_0, axis=0)\n",
    "        var_0 = np.var(x_train_0, axis=0)\n",
    "        avg_1 = np.average(x_train_1, axis=0)\n",
    "        var_1 = np.var(x_train_1, axis=0)\n",
    "        \n",
    "        # Stack the means and variances to fit the (D, 2) shape\n",
    "        hyper_params_0 = np.column_stack((avg_0, var_0))\n",
    "        hyper_params_1 = np.column_stack((avg_1, var_1))\n",
    "        \n",
    "        self.likelihoods_cont_0 = hyper_params_0\n",
    "        self.likelihoods_cont_1 = hyper_params_1\n",
    "        \n",
    "    def find_likelihoods_cat(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Finds the likelihood of each category. Uses Laplace Smoothing to avoid zeros, where the added value\n",
    "        is 1/N, where N is the number of unique values. (Add-1 Smoothing is Laplace Smoothing when the added\n",
    "        value is 1.)\n",
    "        Wiki Article on Laplace Smooting: https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "        \n",
    "        ARGS:\n",
    "            x_train: (N, D) numpy array, data set for the model to be trained on\n",
    "            y_train: (N,) numpy array, labels for x_train\n",
    "        SETS:\n",
    "            self.likelihoods_cat_0: (mC,) numpy array, the likelihood for each category for y = 0, P(x|y = 0)\n",
    "            self.likelihoods_cat_1: (mC,) numpy array, the likelihood for each category for y = 1, P(x|y = 1)\n",
    "        \"\"\"\n",
    "        # Get the number of each label\n",
    "        num_1 = y_train.sum()\n",
    "        num_0 = np.shape(y_train)[0] - num_1\n",
    "        \n",
    "        # Transpose the labels to be vertical\n",
    "        y_train_vert = np.array([y_train]).T\n",
    "        \n",
    "        # Get the data points for each label\n",
    "        x_train_1 = x_train * y_train_vert\n",
    "        x_train_1 = x_train_1[~np.all(x_train_1 == 0, axis=1)]\n",
    "        x_train_0 = x_train * (1 - y_train_vert)\n",
    "        x_train_0 = x_train_0[~np.all(x_train_0 == 0, axis=1)]\n",
    "        \n",
    "        # Isolate the category IDs\n",
    "        x_train_0 = x_train_0[:, 0]\n",
    "        x_train_1 = x_train_1[:, 0]\n",
    "        \n",
    "        # Instantiate the likelihood arrays\n",
    "        likelihood_cat_0 = np.zeros((mC,))\n",
    "        likelihood_cat_1 = np.zeros((mC,))\n",
    "        \n",
    "        # Count the number of each category\n",
    "        for val in x_train_0.astype(int):\n",
    "            likelihood_cat_0[val] += 1\n",
    "        for val in x_train_1.astype(int):\n",
    "            likelihood_cat_1[val] += 1\n",
    "            \n",
    "        # Calculate smoothing value\n",
    "        smoothing_alpha = 1 / C\n",
    "        \n",
    "        # Calculate likelihoods\n",
    "        # Note that the sum will yield >1, but only because there are many unused values\n",
    "        # If summing only the used values, will yield 1\n",
    "        likelihood_cat_0 = (likelihood_cat_0 + smoothing_alpha) / (num_0 + 1)\n",
    "        likelihood_cat_1 = (likelihood_cat_1 + smoothing_alpha) / (num_1 + 1)\n",
    "        \n",
    "        self.likelihood_cat_0 = likelihood_cat_0\n",
    "        self.likelihood_cat_1 = likelihood_cat_1\n",
    "    \n",
    "    def find_likelihoods_mon(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Finds the likelihood of each month. Uses Laplace Smoothing to avoid zeros, where the added value\n",
    "        is 1/N, where N is the number of unique values. (Add-1 Smoothing is Laplace Smoothing when the added\n",
    "        value is 1.)\n",
    "        Wiki Article on Laplace Smooting: https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "        \n",
    "        ARGS:\n",
    "            x_train: (N, D) numpy array, data set for the model to be trained on\n",
    "            y_train: (N,) numpy array, labels for x_train\n",
    "        SETS:\n",
    "            self.likelihoods_mon_0: (M+1,) numpy array, the likelihood for each mon for y = 0, P(x|y = 0)\n",
    "            self.likelihoods_mon_1: (M+1,) numpy array, the likelihood for each mon for y = 1, P(x|y = 1)\n",
    "        \"\"\"\n",
    "        # Get the number of each label\n",
    "        num_1 = y_train.sum()\n",
    "        num_0 = np.shape(y_train)[0] - num_1\n",
    "        \n",
    "        # Transpose the labels to be vertical\n",
    "        y_train_vert = np.array([y_train]).T\n",
    "        \n",
    "        # Get the data points for each label\n",
    "        x_train_1 = x_train * y_train_vert\n",
    "        x_train_1 = x_train_1[~np.all(x_train_1 == 0, axis=1)]\n",
    "        x_train_0 = x_train * (1 - y_train_vert)\n",
    "        x_train_0 = x_train_0[~np.all(x_train_0 == 0, axis=1)]\n",
    "        \n",
    "        # Isolate the months\n",
    "        x_train_0 = x_train_0[:, 1]\n",
    "        x_train_1 = x_train_1[:, 1]\n",
    "        \n",
    "        # Instantiate the likelihood arrays\n",
    "        likelihood_mon_0 = np.zeros((M + 1,))\n",
    "        likelihood_mon_1 = np.zeros((M + 1,))\n",
    "        \n",
    "        # Count the number of each category\n",
    "        for val in x_train_0.astype(int):\n",
    "            likelihood_mon_0[val] += 1\n",
    "        for val in x_train_1.astype(int):\n",
    "            likelihood_mon_1[val] += 1\n",
    "            \n",
    "        # Calculate smoothing value\n",
    "        smoothing_alpha = 1 / M\n",
    "        \n",
    "        # Calculate likelihoods\n",
    "        # Note that the sum will yield >1, but only because there are many unused values\n",
    "        # If summing only the used values, will yield 1\n",
    "        likelihood_cat_0 = (likelihood_mon_0 + smoothing_alpha) / (num_0 + 1)\n",
    "        likelihood_cat_1 = (likelihood_mon_1 + smoothing_alpha) / (num_1 + 1)\n",
    "        \n",
    "        self.likelihood_mon_0 = likelihood_mon_0\n",
    "        self.likelihood_mon_1 = likelihood_mon_1\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "            x_train: (N, D) numpy array, data set for the model to be trained on\n",
    "                Note: Feature 1 (Category) is ordinal and should be treated as such\n",
    "                      All other features are continuous and should be treated as such (Guassian Distribution)\n",
    "            y_train: (N,) numpy array, labels for x_train\n",
    "        SET:\n",
    "            All the instance variables (implicitly)\n",
    "        NOTE:\n",
    "            Since classification is based on relative values, the normalization constant\n",
    "            (the denominator, P(X)) is not needed and does not need to be calculated\n",
    "        \"\"\"\n",
    "        print(\"    Starting to find the priors...\")\n",
    "        self.find_priors(y_train)\n",
    "        print(\"    Completed finding the priors...\\n\")\n",
    "        \n",
    "        print(\"    Starting to find the likelihood of the categories...\")\n",
    "        self.find_likelihoods_cat(x_train, y_train)\n",
    "        print(\"    Completed finding the likelihood of the categories...\\n\")\n",
    "        \n",
    "        print(\"    Starting to find the likelihood of the months...\")\n",
    "        self.find_likelihoods_mon(x_train, y_train)\n",
    "        print(\"    Completed finding the likelihood of the months...\\n\")\n",
    "        \n",
    "        print(\"    Starting to find the likelihoods of the rest of the features...\")\n",
    "        self.find_likelihoods_cont(x_train, y_train)\n",
    "        print(\"    Completed finding the likelihoods of the rest of the features...\")\n",
    "        \n",
    "    def test(self, x_test):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "            x_test: (N, D) numpy array, data set for the model to be tested on\n",
    "                Note: Feature 1 (Category) is ordinal and should be treated as such\n",
    "                      All other features are continuous and should be treated as such (Guassian Distribution)\n",
    "        RETURNS:\n",
    "            y_pred: (N,) numpy array, predicted labels\n",
    "        NOTE:\n",
    "            Since classification is based on relative values, the normalization constant\n",
    "            (the denominator, P(X)) is not needed and does not need to be calculated\n",
    "        \"\"\"\n",
    "        # Get number of data points in the test set\n",
    "        N_test, D_test = np.shape(x_test)\n",
    "        \n",
    "        # Probabilities of each label for each data point\n",
    "        prob = np.zeros((N_test, 2))\n",
    "        \n",
    "        # Start with the priors, P(y)\n",
    "        prob = prob + self.priors\n",
    "        \n",
    "        # Multiply the likelihood of the categories, P(x = category|y)\n",
    "        likelihood_cat_0 = self.likelihood_cat_0[x_test[:, 0].astype(int)]\n",
    "        likelihood_cat_1 = self.likelihood_cat_1[x_test[:, 0].astype(int)]\n",
    "        likelihood_cat = np.column_stack((likelihood_cat_0, likelihood_cat_1))\n",
    "        prob = prob * likelihood_cat\n",
    "        \n",
    "        # Multiply the likelihood of the months, P(x = month|y)\n",
    "        likelihood_mon_0 = self.likelihood_mon_0[x_test[:, 1].astype(int)]\n",
    "        likelihood_mon_1 = self.likelihood_mon_1[x_test[:, 1].astype(int)]\n",
    "        likelihood_mon = np.column_stack((likelihood_mon_0, likelihood_mon_1))\n",
    "        prob = prob * likelihood_mon\n",
    "        \n",
    "        # Multiply the probabilities from the continuous features, P(x = everything else|y)\n",
    "        for i in range(2, D_test):\n",
    "            curr_features = x_test[:, i]\n",
    "            \n",
    "            mean_0 = self.likelihoods_cont_0[i - 2, 0]\n",
    "            std_dev_0 = np.sqrt(self.likelihoods_cont_0[i - 2, 1])\n",
    "            mean_1 = self.likelihoods_cont_1[i - 2, 0]\n",
    "            std_dev_1 = np.sqrt(self.likelihoods_cont_1[i - 2, 1])\n",
    "            \n",
    "            curr_features_prob_0 = sp.stats.norm.pdf(curr_features, mean_0, std_dev_0)\n",
    "            curr_features_prob_1 = sp.stats.norm.pdf(curr_features, mean_1, std_dev_1)\n",
    "            \n",
    "            curr_features_prob = np.column_stack((curr_features_prob_0, curr_features_prob_1))\n",
    "            \n",
    "            prob = prob * curr_features_prob\n",
    "        \n",
    "        # Find predicted labels\n",
    "        y_pred = np.argmax(prob, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def calc_metrics(self, y_test, y_pred):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "            y_test: (N,) numpy array, true labels of the test set\n",
    "            y_pred: (N,) numpy array, predicted labels of the test set\n",
    "        RETURNS:\n",
    "            accuracy: float, accuracy of this Naive Bayes model\n",
    "            precision: float, precision of this Naive Bayes model\n",
    "            recall: float, recall of this Naive Bayes model\n",
    "            f1: float, f1 score of this Naive Bayes model\n",
    "        \"\"\"\n",
    "        # Get number of data points that were tested\n",
    "        N_tested = np.shape(y_test)[0]\n",
    "        \n",
    "        # Get the results as ints\n",
    "        y_test_int = y_test.astype(int)\n",
    "        y_pred_int = y_pred.astype(int)\n",
    "        \n",
    "        # Get the wrong predictions\n",
    "        wrong_preds = np.bitwise_xor(y_test_int, y_pred_int)\n",
    "        \n",
    "        # Find the true/false postives/negatives\n",
    "        t_pos = np.bitwise_and(y_test_int, y_pred_int)\n",
    "        f_pos = np.bitwise_and(y_pred_int, wrong_preds)\n",
    "        t_neg = np.bitwise_and(1 - y_test_int, 1 - y_pred_int)\n",
    "        f_neg = np.bitwise_and(y_test_int, wrong_preds)\n",
    "        \n",
    "        num_tp = t_pos.sum()\n",
    "        num_fp = f_pos.sum()\n",
    "        num_tn = t_neg.sum()\n",
    "        num_fn = f_neg.sum()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (num_tp + num_tn) / (num_tp + num_fp + num_tn + num_fn)\n",
    "        precision = num_tp / (num_tp + num_fp)\n",
    "        recall = num_tp / (num_tp + num_fn)\n",
    "        f1 = (precision * recall) / (precision + recall)\n",
    "        \n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be16b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaning(object):\n",
    "    #class to handle cleaning data (standardizing features, dimensionality reduction, etc.)\n",
    "    \n",
    "    #PCA for dimensionality reduction of the dataset\n",
    "    def pca(self, X, var = False, k=2):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "            X: (N X D) data set as a numpy array, uncentered\n",
    "            var: whether to use retained variance or number of features as the basis of reduction\n",
    "            k: if var=FALSE, k is the number of features to be kept. if var=TRUE, k is the retained variance as a decimal\n",
    "\n",
    "        RETURNS:\n",
    "            new_data: dataset (as a numpy array) obtained by applying PCA on the original dataset\n",
    "        \"\"\"\n",
    "\n",
    "        #center the data set\n",
    "        centeredData = X - np.mean(X, axis=0)\n",
    "        __, S, V = np.linalg.svd(centeredData, full_matrices = False)\n",
    "\n",
    "        #if var== False, do PCA based on the specified number of features\n",
    "        if (var == False):\n",
    "            #if k not entered correctly (less than one feature entered), assume default number of features\n",
    "            if (k < 1):\n",
    "                k = 2\n",
    "            V = V[0:k,:]\n",
    "            new_data = np.matmul(centeredData, V.T)\n",
    "        \n",
    "        #if var==True, do PCA based on the specified number of features\n",
    "        else:\n",
    "            #if k not entered correctly (greater than 100%), assume default variance\n",
    "            if (k >= 1):\n",
    "                k = .99\n",
    "            new_data = np.matmul(centeredData, V.T)\n",
    "            ssquared = np.square(S)\n",
    "            origVar = np.sum(ssquared)\n",
    "            for i in range(np.shape(centeredData)[1]):\n",
    "                var = np.sum(ssquared[0:i])/origVar\n",
    "                if (var >= k):\n",
    "                    V_temp = V[0:i,:]\n",
    "                    new_data= np.matmul(centeredData, V_temp.T)\n",
    "                    break\n",
    "        return new_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc1788",
   "metadata": {},
   "source": [
    "## Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "650bad05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to create the model...\n",
      "Completed creating the model...\n",
      "\n",
      "Starting to run PCA on the data...\n",
      "Completed running PCA on the data...\n",
      "Starting to train the model...\n",
      "    Starting to find the priors...\n",
      "    Completed finding the priors...\n",
      "\n",
      "    Starting to find the likelihood of the categories...\n",
      "    Completed finding the likelihood of the categories...\n",
      "\n",
      "    Starting to find the likelihood of the months...\n",
      "    Completed finding the likelihood of the months...\n",
      "\n",
      "    Starting to find the likelihoods of the rest of the features...\n",
      "    Completed finding the likelihoods of the rest of the features...\n",
      "Completed training the model...\n",
      "\n",
      "Starting to test the model...\n",
      "Completed testing the model...\n",
      "\n",
      "The model had an accuracy of 0.6892.\n",
      "The model had a precision of 0.5739.\n",
      "The model had a recall of 0.8804.\n",
      "The model had an f1 score of 0.3474.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting to create the model...\")\n",
    "nb = NaiveBayes()\n",
    "print(\"Completed creating the model...\\n\")\n",
    "\n",
    "print(\"Starting to run PCA on the data...\")\n",
    "ord_columns = nb.x[:, 0:2]    # Isolate the odrinal data\n",
    "clean = Cleaning()\n",
    "cleaned_x = clean.pca(nb.x[:, 2:], k=4)    # Don't run ordinal data through PCA\n",
    "cleaned_x = np.hstack((ord_columns, cleaned_x))\n",
    "print(\"Completed running PCA on the data...\")\n",
    "\n",
    "# Test on 20% of the dataset\n",
    "test_amount = N // 5\n",
    "x_test = cleaned_x[0:test_amount, :]\n",
    "y_test = nb.y[0:test_amount]\n",
    "x_train = cleaned_x[test_amount:, :]\n",
    "y_train = nb.y[test_amount:]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting to train the model...\")\n",
    "nb.train(x_train, y_train)\n",
    "print(\"Completed training the model...\\n\")\n",
    "\n",
    "# Test the model\n",
    "print(\"Starting to test the model...\")\n",
    "y_pred = nb.test(x_test)\n",
    "print(\"Completed testing the model...\\n\")\n",
    "\n",
    "# Find metrics\n",
    "accuracy, precision, recall, f1 = nb.calc_metrics(y_test, y_pred)\n",
    "print(\"The model had an accuracy of %.4f.\" % accuracy)\n",
    "print(\"The model had a precision of %.4f.\" % precision)\n",
    "print(\"The model had a recall of %.4f.\" % recall)\n",
    "print(\"The model had an f1 score of %.4f.\" % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
